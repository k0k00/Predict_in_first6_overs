{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPL Powerplay Score Prediction\n",
    "![IPL banner](https://cricketaddictor.gumlet.io/wp-content/uploads/2021/02/153757448_173638941016448_6980867142752435675_n.jpg?compress=true&quality=80&w=1920&dpr=2.6)\n",
    "\n",
    "It's COVID lockdown time. Everyone is worried about the increasing COVID cases, Work From Home (WFH) environment. No way to go out, no parties, no outing, no vacation plans, no gatherings ...... \n",
    "\n",
    "Government announces Lockdown : 1.0, 2.0, 3.0, 4.0 .....\n",
    "\n",
    "At that time every cricket fan had a question. \n",
    "> **\"Do new Govt. rules in lockdown 4.0 pave the way for IPL 2020 ????\"**\n",
    "\n",
    "\n",
    "\n",
    "Finally IPL 2020 happend. \n",
    "* **Even pandamic situation can't derail the IPL juggernaut.**\n",
    "* IPL 20202 earned revenue of : 4000 crores with \n",
    "  *  35% reduced cost and \n",
    "  *  25% increase in viewership. \n",
    "\n",
    "As a cricket fan I watch all the matches, during that time I observed that **\"Powerplay plays a mojor role\"** which is very Important in Team score.\n",
    "\n",
    "## What is powerplay in IPL & it's Importance???\n",
    "* In summary \"powerplay\" means fielding restrictions in 1<sup>st</sup> 6-overs.\n",
    "  * means, only 2-fielders can stay outside the Inner-circle\n",
    "  * After powerplay, upto 5-fielders can stay outside inner circle & 4-fielders must remain inside the inner circle.\n",
    "* Which makes the **batting compatively easy.**\n",
    "* Also **it's a trap for the batsmen**, as this will get them to take into a risk and loose their wickets in 1<sup>st</sup> 6-overs\n",
    "\n",
    "## Effect of \"Powerplay\" in IPL matches??\n",
    "* Powerplay overs are considered as pillars of any teams  victory.\n",
    "* 75% - of winning chance of the team depends on the Powerplay score.\n",
    "* So, every team's expectation from the top 3-batsmen is **\"START THE INNINGS BIG\"**\n",
    "\n",
    "In this blog post I am going to predict the score of an IPL match at the end of powerplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,zipfile,io,json\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "#from sklearn.preprocessing import LabelEncoder\r\n",
    "from collections import OrderedDict\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import torch \r\n",
    "\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Variable Declaration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs      = 100\r\n",
    "lr          = 1e-1\r\n",
    "split_ratio = (70.0,30.0)\r\n",
    "train_bs    = 100   # Then test_bs = 2*train_bs\r\n",
    "pltName     = 'test6.png'\r\n",
    "changeDescription   = 'With L1 Loss'\r\n",
    "\r\n",
    "seed = 143\r\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(Path.cwd(),'dataset')\r\n",
    "snippets_path = Path(Path.cwd(),'snippets')\r\n",
    "filename = 'all_matches.csv'\r\n",
    "recordedMetrics = 'Recorded_Metrics.csv'\r\n",
    "\r\n",
    "json_path = Path.joinpath(dataset_path, 'label_encode.json')\r\n",
    "\r\n",
    "save_files = False\r\n",
    "record = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.Loading the Data Set:** \r\n",
    "\r\n",
    "Dataset will be available at : https://cricsheet.org/downloads/ipl_male_csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCsvFile(url=\"https://cricsheet.org/downloads/ipl_male_csv2.zip\"):\r\n",
    "    res = requests.get(url, stream=True)\r\n",
    "    if res.status_code == 200:\r\n",
    "        print('### Downloading the CSV file')\r\n",
    "        z = zipfile.ZipFile(io.BytesIO(res.content))\r\n",
    "        if filename in z.namelist():\r\n",
    "            z.extract(filename, dataset_path)\r\n",
    "            print('### Extracted %s file' % filename)\r\n",
    "        else:\r\n",
    "            print('### %s : File not found in ZIP Artifact' % filename)\r\n",
    "\r\n",
    "def downloadDataset():\r\n",
    "    if not dataset_path.exists():\r\n",
    "        Path.mkdir(dataset_path)\r\n",
    "        print('### Created Dataset folder')\r\n",
    "        getCsvFile()\r\n",
    "    elif dataset_path.exists():\r\n",
    "        files = [file for file in dataset_path.iterdir() if file.name ==\r\n",
    "                 'all_matches.csv']\r\n",
    "        if len(files) == 0:\r\n",
    "            getCsvFile()\r\n",
    "        else:\r\n",
    "            print('### File already extracted in given path')\r\n",
    "downloadDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Data Frames:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file=Path.joinpath(dataset_path, filename)\n",
    "\n",
    "df = pd.read_csv(csv_file,parse_dates=['start_date'],low_memory=False)\n",
    "df_parsed = df.copy(deep=True)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Exploratory Data Analysis (EDA) / Data Processing:**\n",
    "## 3.1: Identify \"null\" values in Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Cleaning Steps:\n",
    "1. Here columns 'season' and 'start_date'  are not needed for our prediction. ```So we can drop the columns 'season' and 'start_date' from the dataset.```\n",
    "2. Delete Non-existing teams : \n",
    "```\n",
    "       'Kochi Tuskers Kerala' 'Pune Warriors','Rising Pune Supergiants', 'Rising Pune Supergiant','Gujarat Lions'\n",
    "```\n",
    "3. Replace the old team names with new team name:\n",
    "```\n",
    "      'Delhi Daredevils'  --> 'Delhi Capitals'\n",
    "      'Deccan Chargers'   --> 'Sunrisers Hyderabad'\n",
    "      'Punjab Kings'      --> 'Kings XI Punjab'\n",
    "```\n",
    "\n",
    "4. Correct the venue column with unique names. In this dataset same stadium is being represented as in multiple ways. So identify those and rename.\n",
    "```\n",
    "      ['M Chinnaswamy Stadium', 'M.Chinnaswamy Stadium']\n",
    "      ['Brabourne Stadium', 'Brabourne Stadium, Mumbai']\n",
    "      ['Punjab Cricket Association Stadium, Mohali', 'Punjab Cricket Association IS Bindra Stadium, Mohali', 'Punjab Cricket Association IS Bindra Stadium']\n",
    "      ['Wankhede Stadium', 'Wankhede Stadium, Mumbai']\n",
    "      ['Rajiv Gandhi International Stadium, Uppal', 'Rajiv Gandhi International Stadium']\n",
    "      ['MA Chidambaram Stadium, Chepauk','MA Chidambaram Stadium',      'MA Chidambaram Stadium, Chepauk, Chennai']\n",
    "```\n",
    "\n",
    "5. Rename the column names (for easy coding)\n",
    "```\n",
    "      'striker'     --> 'batsmen'\n",
    "      'non-striker' --> 'batsmen_nonstriker'  (This column is not required)\n",
    "      'bowler'      --> 'bowlers'\n",
    "```\n",
    "      \n",
    "6. Create a columns \"Total_score\" : which reflects the runs through bat and extra runs through wides,byes,noballs,legbyes...etc.  \n",
    "Hence we can drop columsn ```['wides', 'noballs', 'byes', 'legbyes', 'penalty', 'wicket_type','other_wicket_type', 'other_player_dismissed']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.drop(columns=['wides', 'noballs', 'byes', 'legbyes', 'penalty', 'wicket_type','other_wicket_type', 'other_player_dismissed','season','start_date'],axis=1,inplace=True)\n",
    "df_parsed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_exist_teams = ['Kochi Tuskers Kerala',\n",
    "                    'Pune Warriors',\n",
    "                    'Rising Pune Supergiants',\n",
    "                    'Rising Pune Supergiant',\n",
    "                    'Gujarat Lions']\n",
    "mask_bat_team = df_parsed['batting_team'].isin(non_exist_teams)\n",
    "mask_bow_team = df_parsed['bowling_team'].isin(non_exist_teams)\n",
    "df_parsed = df_parsed[~mask_bat_team]\n",
    "df_parsed = df_parsed[~mask_bow_team]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.loc[df_parsed.batting_team ==\n",
    "                'Delhi Daredevils', 'batting_team'] = 'Delhi Capitals'\n",
    "df_parsed.loc[df_parsed.batting_team == 'Deccan Chargers',\n",
    "                'batting_team'] = 'Sunrisers Hyderabad'\n",
    "df_parsed.loc[df_parsed.batting_team ==\n",
    "                'Punjab Kings', 'batting_team'] = 'Kings XI Punjab'\n",
    "\n",
    "df_parsed.loc[df_parsed.bowling_team ==\n",
    "                'Delhi Daredevils', 'bowling_team'] = 'Delhi Capitals'\n",
    "df_parsed.loc[df_parsed.bowling_team == 'Deccan Chargers',\n",
    "                'bowling_team'] = 'Sunrisers Hyderabad'\n",
    "df_parsed.loc[df_parsed.bowling_team ==\n",
    "                'Punjab Kings', 'bowling_team'] = 'Kings XI Punjab'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.venue.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x : 'chidam' in x.lower(),list(df_parsed.venue.unique())))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.loc[df_parsed.venue == 'M.Chinnaswamy Stadium',\n",
    "                'venue'] = 'M Chinnaswamy Stadium'\n",
    "df_parsed.loc[df_parsed.venue == 'Brabourne Stadium, Mumbai',\n",
    "                'venue'] = 'Brabourne Stadium'\n",
    "df_parsed.loc[df_parsed.venue == 'Punjab Cricket Association IS Bindra Stadium, Mohali',\n",
    "                'venue'] = 'Punjab Cricket Association Stadium'\n",
    "df_parsed.loc[df_parsed.venue == 'Punjab Cricket Association IS Bindra Stadium',\n",
    "                'venue'] = 'Punjab Cricket Association Stadium'\n",
    "df_parsed.loc[df_parsed.venue == 'Wankhede Stadium, Mumbai',\n",
    "                'venue'] = 'Wankhede Stadium'\n",
    "df_parsed.loc[df_parsed.venue == 'Rajiv Gandhi International Stadium, Uppal',\n",
    "                'venue'] = 'Rajiv Gandhi International Stadium'\n",
    "df_parsed.loc[df_parsed.venue == 'MA Chidambaram Stadium, Chepauk',\n",
    "                'venue'] = 'MA Chidambaram Stadium'\n",
    "df_parsed.loc[df_parsed.venue == 'MA Chidambaram Stadium, Chepauk, Chennai',\n",
    "                'venue'] = 'MA Chidambaram Stadium'\n",
    "df_parsed.loc[df_parsed.venue == 'Arun Jaitley Stadium','venue'] = 'Arun Jaitley Stadium, Delhi'\n",
    "df_parsed.loc[df_parsed.venue == 'Punjab Cricket Association Stadium, Mohali','venue'] = 'Punjab Cricket Association Stadium'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = df_parsed.rename(columns={\n",
    "                                 'striker': 'batsmen', 'non_striker': 'batsmen_non_striker', 'bowler': 'bowlers'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed['Total_score'] = df_parsed.runs_off_bat + df_parsed.extras\n",
    "\n",
    "#df_parsed.drop(columns=['wides', 'noballs', 'byes', 'legbyes', 'penalty', 'wicket_type','other_wicket_type', 'other_player_dismissed'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Select required Data:\n",
    "   Here as we are predicting the IPL match **\"Powerplay\"** score, we can drop the rest of the balls and run detail through, \n",
    "   * by selecting the 1<sup>st</sup> 6-Overs details and  \n",
    "   * with Innings (1 and 2) : as there is no 3<sup>rd</sup> or 4<sup>th</sup> innings happen in powerplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = df_parsed[(df_parsed.ball < 6.0) & (df_parsed.innings < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\r\n",
    "    df_parsed.to_csv(Path.joinpath(dataset_path, '1_parseNcleaned_Dataset.csv'), index=False)\r\n",
    "\r\n",
    "print('### Total {} : venue details present '.format(\r\n",
    "    len(df_parsed.venue.unique())))\r\n",
    "print('### Total {}  : Batting teams are there'.format(\r\n",
    "    len(df_parsed.batting_team.unique())))\r\n",
    "print('### Total {}  : Bowlling teams are there'.format(\r\n",
    "    len(df_parsed.bowling_team.unique())))\r\n",
    "print('### Batting teams are : {}'.format(df_parsed.batting_team.unique()))\r\n",
    "print('### Bowling teams are : {}'.format(df_parsed.bowling_team.unique()))\r\n",
    "print('### Shape of data frame after initial cleanup :{}'.format(df_parsed.shape))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_parsed = pd.read_csv(Path.joinpath(dataset_path, '1_parseDataset.csv'))\n",
    "#df_parsed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Encoding:** \n",
    "\n",
    "Dataframe column values of Dtype Object:\n",
    "Till now as a initial step in step-2,\n",
    "* We cleaned our dataset with all the \"null\" values and \n",
    "* filtered the columns/rows data (which are not used for prediction) and\n",
    "* Added the required column with values (like total_score) to make the dataset clean.\n",
    "\n",
    "As you can see above, our cleaned dataset is having 13 columns of multiple Dtype like int64 and float64 and object.\n",
    "\n",
    "So next what I am going to do is trying to convert all these multiple Dtypes into a single Dtype, to train my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1:  Encode \"batsmen\" and \"bowlers\" column values:\n",
    "   * Here we can see, few players who can bat as well as bowl.\n",
    "    Means same player will be listed as a batsmen and as well as bowler.\n",
    "    * So to make the prediction properly, I am creating one Dataframe with all the players name it as \"players_df\", which I use for encoding the players with some value to identify.\n",
    "* For inference going further, I create a dictionary with all these encoded values \n",
    "\n",
    "Use \"sklearn label encoder\" module to encode the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = pd.DataFrame(np.append(\r\n",
    "                            df_parsed.batsmen.unique(), df_parsed.bowlers.unique()),\r\n",
    "                            columns=['Players']\r\n",
    "                        )\r\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encode_dict = {}\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#le.fit(players_df.Players)\n",
    "#Players_e = le.transform(players_df.Players)\n",
    "#Players_e_inv = le.inverse_transform(Players_e)\n",
    "\n",
    "dct = dict(enumerate(players_df.Players.astype('category').cat.categories))\n",
    "label_encode_dict['Players'] = dict(zip(dct.values(),dct.keys()))\n",
    "\n",
    "#label_encode_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: Encode \"vennue\" and \"batting_team\" and \"bowling_team\" column values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_parsed.select_dtypes('object').columns\n",
    "for col in ['venue', 'batting_team', 'bowling_team']:\n",
    "    dct = dict(enumerate(df_parsed[col].astype('category').cat.categories))\n",
    "    label_encode_dict[col] = dict(zip(dct.values(),dct.keys()))\n",
    "\n",
    "label_encode_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encode_dict['batting_team'] == label_encode_dict['bowling_team']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3: Save the encoded values to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, 'w') as f:\n",
    "    json.dump(label_encode_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4: Format the Dataset:\n",
    "In this step I am trying to club the all rows with respect to matchID and Innings (as match ID is unique way to identify a particular match and Innings to identify who bat first).\n",
    "\n",
    "Based on these two details, \n",
    "* grab all the batsmen and bowler details who batted and bowled in 1<sup>st</sup> 6-overs\n",
    "* Calculate the total score (runs through bat + extra runs)\n",
    "* How many players dismissed in 1st 6-overs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Shape of Dataframe before format_data : {}'.format(df_parsed.shape))\n",
    "Runs_off_Bat_6_overs = df_parsed.groupby(['match_id', 'venue', 'innings', 'batting_team', 'bowling_team'])['runs_off_bat'].sum()\n",
    "Extras_6_overs = df_parsed.groupby(['match_id', 'venue', 'innings', 'batting_team', 'bowling_team'])['extras'].sum()\n",
    "TotalScore_6_overs = df_parsed.groupby(['match_id', 'venue', 'innings', 'batting_team', 'bowling_team'])['Total_score'].sum()\n",
    "\n",
    "Total_WktsDown = df_parsed.groupby(['match_id', 'venue', 'innings', 'batting_team', 'bowling_team'])['player_dismissed'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Runs_off_Bat_6_overs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_df = df_parsed.groupby(['match_id', 'venue', 'innings','batting_team', 'bowling_team'])['batsmen'].apply(list)\n",
    "bow_df = df_parsed.groupby(['match_id', 'venue', 'innings','batting_team', 'bowling_team'])['bowlers'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now concat all these formated data into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = pd.DataFrame(pd.concat([bat_df, bow_df, Runs_off_Bat_6_overs, Extras_6_overs, TotalScore_6_overs, Total_WktsDown],axis=1)).reset_index()\r\n",
    "\r\n",
    "if save_files:\r\n",
    "    df_parsed.to_csv(Path.joinpath(dataset_path,'2_powerplay_summary.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 : Align the batsmen and bowlers details in to a separate column\n",
    "\n",
    "In above formated dataset, we got list of batsmen and bowlers details who batted and bowled in 6-overs.\n",
    "\n",
    "Now we have to arrange these batsmen into a separate columns,\n",
    "```\n",
    " * say bat1,bat2,bat3,bat4....bat10\n",
    " * say bow1,bow2,bow3.....bow6\n",
    "```\n",
    "\n",
    "Here I selected only 10-batsmen (as we have only 10-wickets), and 6-bowlers (can bowl in 6-overs) because in 6-overs this is only possible.\n",
    "\n",
    "```\n",
    "For proper prediction the order of batsmen and bowlers given the dataset matters. So we need to keep the order :\n",
    "* batsmen : who batted 1st,2nd 3rd and 4th ... wicket same\n",
    "* bowler  : who bowled in 1st,2nd,3rd,4th,5th and 6th overs same\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6: Create a batsmen and bowlers dummy dataframe:\n",
    "Inorder to keep track of the order same, so 1st I am going to create a dummy dataframe \n",
    "* with 10-batsmen with column names [bat1,bat2,.....bat9,bat10]\n",
    "* with 6-bowlers with column names [bow1,bow2,bow3,bow4,bow5,bow6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat  = pd.DataFrame(np.zeros((df_parsed.shape[0],10),dtype=float),columns=['bat1', 'bat2', 'bat3', 'bat4', 'bat5', 'bat6', 'bat7', 'bat8', 'bat9', 'bat10'])\n",
    "bowl = pd.DataFrame(np.zeros((df_parsed.shape[0],6),dtype=float),columns=['bow1', 'bow2', 'bow3', 'bow4', 'bow5', 'bow6'])\n",
    "\n",
    "columns = ['bat1', 'bat2', 'bat3', 'bat4', 'bat5', 'bat6', 'bat7', 'bat8', 'bat9', 'bat10','bow1', 'bow2', 'bow3', 'bow4', 'bow5', 'bow6']\n",
    "df_bat_bow = pd.concat([bat,bowl],axis=1)\n",
    "df_bat_bow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed =  pd.concat([df_parsed, df_bat_bow], axis=1)\r\n",
    "if save_files:\r\n",
    "    df_parsed.to_csv(Path.joinpath(dataset_path, '3_create_batsmen_bowler_df.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7: Update the batsmen list of elements into each column in the same order:\n",
    "\n",
    "Means, data which is in list of elements in each matchID and innings into corresponding individual batsmen columns in the same order. \n",
    "```\n",
    "Example, here in below list 1st batsmen is bat1 -> SC Ganguly, 2nd is bat2 -> BB McCullum, 3rd is bat3 -> RT Poting ...etc like \n",
    "\n",
    "\"['SC Ganguly', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'SC Ganguly', 'SC Ganguly', 'SC Ganguly', 'BB McCullum', 'BB McCullum', 'SC Ganguly', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'SC Ganguly', 'SC Ganguly', 'SC Ganguly', 'BB McCullum', 'SC Ganguly', 'SC Ganguly', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'SC Ganguly', 'BB McCullum', 'SC Ganguly', 'RT Ponting', 'RT Ponting', 'RT Ponting', 'RT Ponting', 'BB McCullum', 'RT Ponting', 'BB McCullum', 'RT Ponting', 'RT Ponting', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'RT Ponting', 'BB McCullum', 'RT Ponting', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'RT Ponting', 'BB McCullum', 'RT Ponting', 'BB McCullum', 'RT Ponting', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'RT Ponting', 'RT Ponting', 'RT Ponting', 'RT Ponting', 'RT Ponting', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'RT Ponting', 'RT Ponting', 'RT Ponting', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'DJ Hussey', 'DJ Hussey', 'BB McCullum', 'DJ Hussey', 'BB McCullum', 'DJ Hussey', 'DJ Hussey', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'DJ Hussey', 'BB McCullum', 'DJ Hussey', 'DJ Hussey', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'DJ Hussey', 'BB McCullum', 'DJ Hussey', 'DJ Hussey', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'DJ Hussey', 'BB McCullum', 'Mohammad Hafeez', 'Mohammad Hafeez', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'Mohammad Hafeez', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum', 'BB McCullum']\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed['batsmen'] = df_parsed.batsmen.map(OrderedDict.fromkeys).apply(list)\n",
    "df_parsed['bowlers'] = df_parsed.bowlers.map(OrderedDict.fromkeys).apply(list)\n",
    "df_parsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row,val in enumerate(df_parsed.batsmen):\r\n",
    "    for i in range(len(val)):\r\n",
    "        df_parsed.loc[row,'bat%s'%(i+1)] = val[i]\r\n",
    "\r\n",
    "for row,val in enumerate(df_parsed.bowlers):\r\n",
    "    for i in range(len(val)):\r\n",
    "        df_parsed.loc[row,'bow%s'%(i+1)] = val[i]\r\n",
    "\r\n",
    "df_parsed.loc[:,['bat1','bat2','bat3','bat4','bat5','bat6','bat7','bat8','bat9','bat10','bow1','bow2','bow3','bow4','bow5','bow6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\r\n",
    "    df_parsed.to_csv(Path.joinpath(dataset_path,'4_update_batsmen_bowler_column_names.csv'),index=False)\r\n",
    "#df_parsed = pd.read_csv(Path.joinpath(dataset_path, '4_update_batsmen_bowler_column_names.csv'))\r\n",
    "df_parsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally our dataframe is ready with batsmen and bowlers details.\n",
    "\n",
    "So we can drop few columns which are not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_parsed[['venue', 'innings', 'batting_team', 'bowling_team', 'bat1', 'bat2', 'bat3', 'bat4', 'bat5', 'bat6', 'bat7', 'bat8','bat9', 'bat10', 'bow1', 'bow2', 'bow3', 'bow4', 'bow5', 'bow6', 'runs_off_bat', 'extras', 'Total_score', 'player_dismissed']]\n",
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_df_to_markdown_table(df):\n",
    "    from IPython.display import Markdown, display\n",
    "    fmt = ['---' for i in range(len(df.columns))]\n",
    "    df_fmt = pd.DataFrame([fmt], columns=df.columns)\n",
    "    df_formatted = pd.concat([df_fmt, df])\n",
    "    display(Markdown(df_formatted.to_csv(sep=\"|\", index=False)))\n",
    "\n",
    "pandas_df_to_markdown_table(df_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8: Encode the multiple Dtypes into single Dtype:\r\n",
    "\r\n",
    "Now its time to use, the label encoded values (already done in previous steps) to encode the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = Path.joinpath(dataset_path, 'label_encode.json')\r\n",
    "with open(json_path) as f:\r\n",
    "    data = json.load(f)\r\n",
    "\r\n",
    "condition = False\r\n",
    "\r\n",
    "for col in df_model.columns:\r\n",
    "    if col in data.keys():\r\n",
    "        condition = True\r\n",
    "        col = col\r\n",
    "    elif col in ['bat1', 'bat2','bat3', 'bat4', 'bat5', 'bat6', 'bat7', 'bat8', 'bat9', 'bat10']:\r\n",
    "        condition = True\r\n",
    "        col = 'Players' #'batsmen'\r\n",
    "    elif col in ['bow1','bow2', 'bow3', 'bow4', 'bow5', 'bow6']:\r\n",
    "        col = 'Players' #'bowlers'\r\n",
    "        condition = True\r\n",
    "\r\n",
    "    if condition:\r\n",
    "        condition = False\r\n",
    "        for key in data[col]:\r\n",
    "            df_model = df_model.replace([key], data[col][key])\r\n",
    "\r\n",
    "if save_files:\r\n",
    "    df_model.to_csv(Path.joinpath(dataset_path, '5_model_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally dataset is ready. \n",
    "Next step is prepare model for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_to_markdown_table(df_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Normalization:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert entire dataframe into flot point numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.applymap(np.float64)\r\n",
    "df_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here initially our dataset is of size ~ 30MB, now its in 273 KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.iloc[0:4,5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using **Min Max Scaling** to normalize the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = (df_model - df_model.min())/(df_model.max() - df_model.min())\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.fillna(0.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm[['bat9','bat10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.iloc[0:4,4:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Split Train & test data**\r\n",
    "\r\n",
    "Split the dataset into 70% for train and 30% for test\r\n",
    "![splitDatasetImg](https://miro.medium.com/max/1400/1*n7Ob33nRMq07BZPbNtfItw.png)\r\n",
    "\r\n",
    "1. Identify the inputs vs targets from the pandas dataframe  and convert into Torch Tensor\r\n",
    "2. Create the torch dataset\r\n",
    "3. Now split the torch_ds into train_ds and test_ds datasets (70% vs 30%)\r\n",
    "4. Create a dataloader for train_ds and test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.Tensor(df_norm.iloc[:,:-4].values.astype(np.float32))\r\n",
    "targets = torch.Tensor(df_norm.loc[:,'Total_score'].values.reshape(-1,1).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_ds = torch.utils.data.TensorDataset(inputs,targets)\r\n",
    "len(torch_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_sz = int(round((len(torch_ds)*split_ratio[0])/100.0))\r\n",
    "test_ds_sz  = len(torch_ds) - train_ds_sz\r\n",
    "\r\n",
    "train_ds,test_ds = torch.utils.data.random_split(torch_ds,[train_ds_sz,test_ds_sz])\r\n",
    "len(train_ds),len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(dataset = train_ds,batch_size=train_bs, shuffle = True)\r\n",
    "test_dl = torch.utils.data.DataLoader(dataset = test_ds,batch_size=2*train_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. MODEL Selection:**\r\n",
    "\r\n",
    "Here I am using Linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(inputs.shape[1],targets.shape[1])\r\n",
    "\r\n",
    "model.weight.shape,model.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Train & Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\r\n",
    "\r\n",
    "train_loss, test_loss = [],[]\r\n",
    "def train(epoch,model,train_loader,optimizer,loss):\r\n",
    "    model.train()\r\n",
    "    pbar = tqdm(train_loader)\r\n",
    "    for batch_idx,(data,target) in enumerate(pbar):\r\n",
    "        optimizer.zero_grad()\r\n",
    "        output = model(data)\r\n",
    "        error = loss(output,target)\r\n",
    "        error.backward()\r\n",
    "        optimizer.step()\r\n",
    "        #if epoch % 10 == 0:\r\n",
    "        pbar.set_description(desc= f'Epoch : {epoch} : Train - loss={error.item():.4f} batch_id={batch_idx}')\r\n",
    "    train_loss.append(error.item())\r\n",
    "\r\n",
    "\r\n",
    "def test(epoch,model,test_loader,loss):\r\n",
    "    model.eval()\r\n",
    "    pbar = tqdm(test_loader)\r\n",
    "    with torch.no_grad():\r\n",
    "        for batch_idx,(data,target) in enumerate(pbar):\r\n",
    "            output = model(data)\r\n",
    "            error = loss(output,target)\r\n",
    "            #if epoch %10 ==0:\r\n",
    "            pbar.set_description(desc= f'Epoch : {epoch} : Test - loss={error.item():.4f} batch_id={batch_idx}')\r\n",
    "        test_loss.append(error.item())\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Loss & optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.L1Loss()\r\n",
    "opt  = torch.optim.SGD(model.parameters(),lr=lr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Train Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\r\n",
    "    train(epoch,model = model, train_loader = train_dl, optimizer = opt,loss = loss)\r\n",
    "    test(epoch,model = model,  test_loader = test_dl, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.1. Train vs Test Loss Graph:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\r\n",
    "plt.plot(test_loss);\r\n",
    "plt.title('Train vs Test Loss');\r\n",
    "plt.xlabel('epoch');\r\n",
    "plt.ylabel('Loss');\r\n",
    "plt.legend(['Train_Loss','Test_Loss']);\r\n",
    "plt.savefig(pltName);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\r\n",
    "for i,j in test_dl:\r\n",
    "    test_inp  = i[0]\r\n",
    "    test_op   = j[0] \r\n",
    "    print(test_inp,test_op)\r\n",
    "    break\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = torch.Tensor([0.6562, 1.0000, 0.4286, 0.2857, 0.1336, 0.2767, 0.5742, 0.0000, 0.0000,\r\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6360, 0.0511, 0.3145, 0.6621,\r\n",
    "        0.0000, 0.0000])\r\n",
    "\r\n",
    "test_op = torch.Tensor([0.4951])\r\n",
    "actual    = (test_op*(df_model.Total_score.max() - df_model.Total_score.min() )) + df_model.Total_score.min()\r\n",
    "model.eval()\r\n",
    "pred     = (model(torch.Tensor(test_inp))*(df_model.Total_score.max() - df_model.Total_score.min() )) + df_model.Total_score.min()\r\n",
    "print(f'Predicted Value : {pred.item()} vs Actual value : {actual.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9. Record metrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\r\n",
    "    'pltName'       : [pltName],\r\n",
    "    'Model'         : ['nn.Linear(20,1)'],\r\n",
    "    'Epochs'        : [epochs],\r\n",
    "    'Loss Func'     : ['L1 Loss'],\r\n",
    "    'Optimizer'     : ['SGD(model.parameters(),lr=lr)'],\r\n",
    "    'LR'            : [lr],\r\n",
    "    'Train vs Test BatchSz' : [(train_bs, 2*train_bs)],\r\n",
    "    'Train Loss'    : [list(map(lambda x: float(f'{x:.4f}'), train_loss))],\r\n",
    "    'Test Loss'     : [list(map(lambda x: float(f'{x:.4f}'), test_loss))],\r\n",
    "    'Test Input'    : [test_inp],\r\n",
    "    'Actual Score'  : [actual.item()],\r\n",
    "    'Predicted'     : [pred.item()],\r\n",
    "    'Notes'         : [changeDescription]\r\n",
    "\r\n",
    "}\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if record:\r\n",
    "    if Path.joinpath(Path.cwd(),recordedMetrics).exists():\r\n",
    "        df = pd.read_csv(recordedMetrics)\r\n",
    "        print('### Concatinating the Metrics to existing Dataframe')\r\n",
    "        df_new = pd.concat([df,pd.DataFrame(metrics)],axis=0)\r\n",
    "    else:\r\n",
    "        print('### Created a new file')\r\n",
    "        df_new = pd.DataFrame(metrics)\r\n",
    "\r\n",
    "    df_new.to_csv(Path.joinpath(Path.cwd(),recordedMetrics),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(recordedMetrics,usecols=['Epochs','LR','Train Loss','Test Loss']) # 'Actual Score', 'Predicted'\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=df['Train Loss'],data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77dcdf196944021a492e8af9322675174f43bd84fa8cdc1beb5f85b333fd20f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}